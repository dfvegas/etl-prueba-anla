# üß© Proyecto ETL: Kaggle ‚Üí Bronze ‚Üí Silver (PostgreSQL)

Este proyecto implementa un pipeline ETL completo utilizando Python, Pandas, SQLAlchemy y PostgreSQL. Se trabaja con el dataset de tipo **Superstore** de Kaggle , que primero entra a una capa Bronze (datos crudos) y luego se transforma a una capa Silver (modelo relacional normalizado).

Incluye:

- Descarga autom√°tica desde Kaggle.
- Limpieza y estandarizaci√≥n de datos (Bronze).
- Normalizaci√≥n completa a un modelo dimensional (Silver).
- Scripts SQL para la creaci√≥n del modelo.
- Tests automatizados con pytest.

---

# 1. Descripci√≥n de la soluci√≥n

El pipeline ETL sigue una arquitectura cl√°sica de **Bronze ‚Üí Silver**. La soluci√≥n implementa una arquitectura modular basada en principios de dise√±o limpio. Cada componente del pipeline cumple una √∫nica responsabilidad (SRP ‚Äì SOLID), permitiendo desacoplar la lectura, limpieza, transformaci√≥n y carga de datos. El modelo Silver adopta un enfoque dimensional, empleando dimensiones con claves surrogate para garantizar estabilidad ante cambios en los identificadores naturales del dataset. La inclusi√≥n de `dim_date` responde a buenas pr√°cticas de modelos anal√≠ticos: facilita consultas por per√≠odos, optimiza √≠ndices y estandariza la gesti√≥n temporal. Asimismo, la dimensi√≥n geogr√°fica se mantiene sin normalizar sus niveles internos (pa√≠s, estado, ciudad) porque el dataset Superstore no presenta jerarqu√≠as complejas ni cardinalidades suficientes que justifiquen dividirla en dimensiones separadas; consolidarlas en una sola tabla evita joins innecesarios y mejora la eficiencia en consultas t√≠picas de an√°lisis. Finalmente, la organizaci√≥n del c√≥digo en m√≥dulos (utils/, etl/, config/) permite reutilizaci√≥n, pruebas unitarias aisladas y una estructura clara, escalable y alineada con patrones comunes en proyectos ETL.

La soluci√≥n actual est√° orientada a funciones para mantenerla simple y f√°cil de leer.
En un escenario real con m√°s fuentes de datos y m√∫ltiples pipelines, evolucionar√≠a esto hacia una arquitectura m√°s orientada a objetos, usando:
- Una clase base de ETL (Template Method) con los pasos extract ‚Üí transform ‚Üí load
- Estrategias de validaci√≥n configurables por dataset.
- Repositorios para abstracci√≥n de acceso a datos.

## Capa Bronze
Contiene los datos crudos provenientes del CSV descargado desde Kaggle.  
Se aplica limpieza m√≠nima para asegurar consistencia:

- Tipos de datos correctos.
- Normalizaci√≥n de nombres de columnas.
- Conversi√≥n de fechas.
- Marcado del archivo fuente y timestamp de ingesta.

## Capa Silver
Modelo dimensional normalizado compuesto por:

- `dim_customer`
- `dim_geography`
- `dim_product`
- `dim_ship_mode`
- `dim_date`
- `fact_orders`
- `fact_order_items`

Caracter√≠sticas principales:

- Llaves surrogate en todas las dimensiones.
- Relaciones por claves for√°neas.
- Separaci√≥n clara entre entidades del negocio.
- Fechas normalizadas v√≠a tabla `dim_date`.
- Hechos con granularidad adecuada:
  - Nivel pedido (`fact_orders`)
  - Nivel √≠tem (`fact_order_items`)

---

# 2. Supuestos tomados

1. El dataset Superstore no tiene duplicados cr√≠ticos en sus llaves naturales.
2. `product_id` identifica un producto, incluso si hay peque√±as inconsistencias textuales.
3. Todos los pedidos tienen cliente, fecha de orden y fecha de env√≠o v√°lidas.
4. Las fechas del CSV pueden convertirse limpiamente a tipo `DATE`.
5. Las bases de datos Bronze y Silver existen en la misma instancia de PostgreSQL.

---

# 3. Preparaci√≥n del entorno

## 3.1 Crear entorno virtual
```bash
python -m venv venv
source venv/Scripts/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

## 3.2 Crear entorno virtual
```bash
pip install -r requirements.txt
```
## 3.2 Configurar variables de entorno
Crear archivo `.env`   
```bash
PGUSER=postgres
PGPASSWORD=admin
PGHOST=localhost
PGPORT=5432

PGDATABASE_BRONZE=bronze
BRONZE_DATABASE_URL=silver
```
---

# 4. Creaci√≥n de bases de datos

Ejecutar en psql:
```bash
CREATE DATABASE bronze;
CREATE DATABASE silver;
```

---

# 5. Ejecutar scripts SQL

Ejecutar en psql:
```bash
psql -U postgres -d bronze -f bronze_schema.sql
psql -U postgres -d silver -f silver_schema.sql
```
---

# 6. Ejecuci√≥n del pipeline ETL

## 6.1 Descarga de la base de datos
Ejecutar el archivo `download_dataset.py` para descargar el dataset.
```bash
python etl/download_dataset.py
```
Esto devuelve una ruta que usaremos en el siguiente paso. El CSV quedar√° almacenado en la cach√© local de kagglehub, por ejemplo:
```bash
C:\Users\<usuario>\.cache\kagglehub\datasets\vivek468\superstore-dataset-final\versions\1\Superstore.csv
```


## 6.2 ETL CSV ‚Üí Bronze
Ejecutar ETL usando la ruta d√≥nde se guarda el archivo descargado desde Kagglehub
```bash
python -m etl.etl_csv_to_bronze "C:\Users\<usuario>\.cache\kagglehub\datasets\vivek468\superstore-dataset-final\versions\1\Superstore.csv"
```

## 6.3 ETL Bronze ‚Üí Silver
```bash
python -m etl.etl_bronze_to_silver
```

---

# 7. Tests automatizados

## 7.1 Ejecutar tests
```bash
pytest -vv
```
## 7.2 ¬øQu√© validan los tests?
- Que `test_etl_csv_to_bronze` valida limpieza y mapeo del CSV.
- Que `test_dimensions` valida la creaci√≥n correcta de las dimensiones.
- Que `test_facts` valida que los hechos se construyen correctamente.
Los tests usan dataframes peque√±os y controlados (unit tests).