# üß© Proyecto ETL: Kaggle ‚Üí Bronze ‚Üí Silver (PostgreSQL)

Este proyecto implementa un pipeline ETL completo utilizando Python, Pandas, SQLAlchemy y PostgreSQL. Se trabaja con el dataset de tipo **Superstore** de Kaggle , que primero entra a una capa Bronze (datos crudos) y luego se transforma a una capa Silver (modelo relacional normalizado).

Incluye:

- Descarga autom√°tica desde Kaggle.
- Limpieza y estandarizaci√≥n de datos (Bronze).
- Normalizaci√≥n completa a un modelo dimensional (Silver).
- Scripts SQL para la creaci√≥n del modelo.
- Tests automatizados con pytest.

---

# 1. Descripci√≥n de la soluci√≥n

El pipeline ETL sigue una arquitectura cl√°sica de **Bronze ‚Üí Silver**.

## Capa Bronze
Contiene los datos crudos provenientes del CSV descargado desde Kaggle.  
Se aplica limpieza m√≠nima para asegurar consistencia:

- Tipos de datos correctos.
- Normalizaci√≥n de nombres de columnas.
- Conversi√≥n de fechas.
- Marcado del archivo fuente y timestamp de ingesta.

## Capa Silver
Modelo dimensional normalizado compuesto por:

- `dim_customer`
- `dim_geography`
- `dim_product`
- `dim_ship_mode`
- `dim_date`
- `fact_orders`
- `fact_order_items`

Caracter√≠sticas principales:

- Llaves surrogate en todas las dimensiones.
- Relaciones por claves for√°neas.
- Separaci√≥n clara entre entidades del negocio.
- Fechas normalizadas v√≠a tabla `dim_date`.
- Hechos con granularidad adecuada:
  - Nivel pedido (`fact_orders`)
  - Nivel √≠tem (`fact_order_items`)

---

# 2. Supuestos tomados

1. El dataset Superstore no tiene duplicados cr√≠ticos en sus llaves naturales.
2. `product_id` identifica un producto, incluso si hay peque√±as inconsistencias textuales.
3. Todos los pedidos tienen cliente, fecha de orden y fecha de env√≠o v√°lidas.
4. Las fechas del CSV pueden convertirse limpiamente a tipo `DATE`.
5. Las bases de datos Bronze y Silver existen en la misma instancia de PostgreSQL.

---

# 3. Preparaci√≥n del entorno

## 3.1 Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

## 3.2 Crear entorno virtual
```bash
pip install -r requirements.txt
```
## 3.2 Configurar variables de entorno
Crear archivo `.env`   
```bash
PGUSER=postgres
PGPASSWORD=admin
PGHOST=localhost
PGPORT=5432

PGDATABASE_BRONZE=bronze
BRONZE_DATABASE_URL=silver
```
---

# 4. Creaci√≥n de bases de datos

Ejecutar en psql:
```bash
CREATE DATABASE bronze;
CREATE DATABASE silver;
```

---

# 5. Ejecutar scripts SQL

Ejecutar en psql:
```bash
psql -U postgres -d bronze -f sql/bronze_schema.sql
psql -U postgres -d silver -f sql/silver_schema.sql
```
---

# 6. Ejecuci√≥n del pipeline ETL

## 6.1 Descarga de la base de datos
Ejecutar el archivo `download_dataset.py` para descargar el dataset.
```bash
python download_dataset.py
```
Esto devuelve una ruta que usaremos en el siguiente paso. El CSV quedar√° almacenado en la cach√© local de kagglehub, por ejemplo:
```bash
C:\Users\<usuario>\.cache\kagglehub\datasets\vivek468\superstore-dataset-final\versions\1\Superstore.csv
```


## 6.2 ETL CSV ‚Üí Bronze
Ejecutar ETL usando la ruta d√≥nde se guarda el archivo descargado desde Kagglehub
```bash
python -m etl.etl_csv_to_bronze "C:\Users\<usuario>\.cache\kagglehub\datasets\vivek468\superstore-dataset-final\versions\1\Superstore.csv"
```

## 6.3 ETL Bronze ‚Üí Silver
```bash
python -m etl.etl_bronze_to_silver
```

---

# 7. Tests automatizados

## 7.1 Ejecutar tests
```bash
pytest -vv
```
## 7.2 ¬øQu√© validan los tests?
- Que `test_etl_csv_to_bronze` valide limpieza y mapeo del CSV.
- Que `test_dimensions` valide la creaci√≥n correcta de las dimensiones.
- Que `test_facts` valide que los hechos se construyen correctamente.
Los tests usan dataframes peque√±os y controlados (unit tests).